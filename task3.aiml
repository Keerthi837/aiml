import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import seaborn as sns

# 1. Import and preprocess the dataset
df = pd.read_csv('house prices.csv')  # Replace 'house_prices.csv' with your dataset file name
df = df.dropna()  # Handle missing values by removing rows with NaN (other strategies are possible)

# 2. Simple Linear Regression (example using 'GrLivArea' to predict 'SalePrice')
X_simple = df[['GrLivArea']]
y_simple = df['SalePrice']
X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(X_simple, y_simple, test_size=0.2, random_state=42)

model_simple = LinearRegression()
model_simple.fit(X_train_simple, y_train_simple)
y_pred_simple = model_simple.predict(X_test_simple)

mae_simple = mean_absolute_error(y_test_simple, y_pred_simple)
mse_simple = mean_squared_error(y_test_simple, y_pred_simple)
r2_simple = r2_score(y_test_simple, y_pred_simple)

print("Simple Linear Regression Results:")
print(f"  MAE: {mae_simple:.2f}")
print(f"  MSE: {mse_simple:.2f}")
print(f"  R²: {r2_simple:.2f}")
print(f"  Intercept: {model_simple.intercept_:.2f}")
print(f"  Coefficient: {model_simple.coef_[0]:.2f}")

# Plotting the simple linear regression line
plt.figure(figsize=(10, 5))
plt.scatter(X_test_simple, y_test_simple, color='blue', label='Actual')
plt.plot(X_test_simple, y_pred_simple, color='red', label='Predicted')
plt.xlabel('GrLivArea')
plt.ylabel('SalePrice')
plt.title('Simple Linear Regression: GrLivArea vs SalePrice')
plt.legend()
plt.show()

# 3. Multiple Linear Regression (using multiple features to predict 'SalePrice')
features = ['GrLivArea', 'TotalBsmtSF', 'GarageArea', 'YearBuilt'] # Example features, can be adjusted
X_multiple = df[features]
y_multiple = df['SalePrice']
X_train_multiple, X_test_multiple, y_train_multiple = train_test_split(X_multiple, y_multiple, test_size=0.2, random_state=42)

model_multiple = LinearRegression()
model_multiple.fit(X_train_multiple, y_train_multiple)
y_pred_multiple = model_multiple.predict(X_test_multiple)

mae_multiple = mean_absolute_error(y_test_multiple, y_pred_multiple)
mse_multiple = mean_squared_error(y_test_multiple, y_pred_multiple)
r2_multiple = r2_score(y_test_multiple, y_pred_multiple)

print("\nMultiple Linear Regression Results:")
print(f"  MAE: {mae_multiple:.2f}")
print(f"  MSE: {mse_multiple:.2f}")
print(f"  R²: {r2_multiple:.2f}")
print("  Intercept:", model_multiple.intercept_)
print("  Coefficients:", model_multiple.coef_)

# 4. Visualize actual vs predicted values for multiple regression
plt.figure(figsize=(10, 5))
plt.scatter(y_test_multiple, y_pred_multiple, color='green')
plt.plot([y_test_multiple.min(), y_test_multiple.max()], [y_test_multiple.min(), y_test_multiple.max()], 'k--')
plt.xlabel('Actual Sale Price')
plt.ylabel('Predicted Sale Price')
plt.title('Multiple Linear Regression: Actual vs Predicted')
plt.show()

# 5. Check for multicollinearity (example using VIF)
from statsmodels.stats.outliers_influence import variance_inflation_factor
X_vif = df[['GrLivArea', 'TotalBsmtSF', 'GarageArea']] # Include only continuous variables for VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]
print("\nVariance Inflation Factor (VIF):")
print(vif_data)

# 6. Residual analysis
residuals = y_test_multiple - y_pred_multiple

plt.figure(figsize=(10, 5))
sns.residplot(x=y_pred_multiple, y=residuals, lowess=True, color="blue")
plt.title('Residual Plot')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.show()